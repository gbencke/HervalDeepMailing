{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import dateutil.parser as parser\n",
    "import os.path\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proporcao_train = 0.8\n",
    "proporcao_test = 0.2\n",
    "\n",
    "log_location = \"../logs/\"\n",
    "arquivo_pagantes_norm = \"../data/intermediate/Herval.normalized.pickle\"\n",
    "arquivo_pagantes_norm_train_x = \"../data/intermediate/Herval.normalized.train.x.pickle\"\n",
    "\n",
    "txt_dump_model = \"../data/intermediate/model_generated.txt\"\n",
    "txt_feat_map_model = \"../data/intermediate/feat_map_generated.txt\"\n",
    "output_folder= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'xgboost.log.' + datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(msg):\n",
    "    logging.debug(msg)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Carregando Pickling normalizado:{}\".format(arquivo_pagantes_norm))    \n",
    "pagantes = pd.read_pickle(arquivo_pagantes_norm)\n",
    "pagantes = pagantes. query('NORM_CONTRATO_ATRASO < 0.15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pagantes.NORM_CONTRATO_ATRASO \n",
    "\n",
    "rcParams['figure.figsize'] = 10,10\n",
    "df4.plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pagantes = len(pagantes.index)\n",
    "print_log(\"Total pagantes:{}\".format(total_pagantes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes = pagantes[['NORM_IDADE', 'NORM_VALOR_DIVIDA', 'NORM_CONTRATO_ATRASO', 'PAGOU']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_reference(header_chamadas_x,arquivo_df_pickled_norm_train_x):\n",
    "    print_log(\"Criando Arquivo de referencia de colunas...\")\n",
    "    with open(arquivo_df_pickled_norm_train_x+\".txt\",\"w\") as f:\n",
    "        counter = 0\n",
    "        lista_header = list(header_chamadas_x.columns.values)\n",
    "        for header in lista_header:\n",
    "            f.write(\"{}-{}\\n\".format(counter,header))\n",
    "            counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes.head(10)\n",
    "#pagantes.loc[:, :'NORM_CONTRATO_ATRASO'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Criando dataframes de train e teste...\")\n",
    "pagantes = pagantes.sample(int(len(pagantes.index)))\n",
    "pagantes_train = pagantes.tail(int(len(pagantes.index) * proporcao_train))\n",
    "pagantes_test = pagantes.head(int(len(pagantes.index) * proporcao_test))\n",
    "del pagantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_column_reference(pagantes_train.loc[:, :'NORM_CONTRATO_ATRASO'].head(1), arquivo_pagantes_norm_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes_train_x = pagantes_train.loc[:, :'NORM_CONTRATO_ATRASO']\n",
    "pagantes_train_y = pagantes_train.loc[:, 'PAGOU':'PAGOU']\n",
    "\n",
    "pagantes_test_x = pagantes_test.loc[:, :'NORM_CONTRATO_ATRASO']\n",
    "pagantes_test_y = pagantes_test.loc[:, 'PAGOU':'PAGOU']\n",
    "\n",
    "colunas_x = pagantes_train_x.columns.values\n",
    "colunas_y = pagantes_train_y.columns.values\n",
    "\n",
    "pagantes_train_x = pagantes_train_x.as_matrix()\n",
    "pagantes_train_y = pagantes_train_y.as_matrix()\n",
    "\n",
    "pagantes_test_x = pagantes_test_x.as_matrix()\n",
    "pagantes_test_y = pagantes_test_y.as_matrix()\n",
    "\n",
    "colunas_x = [x for x in colunas_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = \"Train - Pagantes Detectados {} num universo de {}\".format(len([y for y in pagantes_train_y if y >0]),len(pagantes_train_y))\n",
    "msg2 = \"Test - Pagantes Detectados {} num universo de {}\".format(len([y for y in pagantes_test_y if y >0]),len(pagantes_test_y))\n",
    "print_log(msg1)\n",
    "print_log(msg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['eta'] = 0.3\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'auc'\n",
    "param['tree_method'] = 'exact'\n",
    "param['silent'] = 0\n",
    "param['max_depth'] = 10\n",
    "num_round = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print_log(\"Starting model for params:{}\".format(param))\n",
    "dtrain = xgb.DMatrix(pagantes_train_x, pagantes_train_y, feature_names = colunas_x)\n",
    "dtest = xgb.DMatrix(pagantes_test_x, pagantes_test_y, feature_names = colunas_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dtrain.get_label()\n",
    "ratio = float(np.sum(train_labels == 0)) / np.sum(train_labels == 1) \n",
    "param['scale_pos_weight'] = ratio\n",
    "print_log(\"ratio:{}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_res = {}\n",
    "booster = xgb.train(param, dtrain, num_round, evals=[], evals_result=gpu_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster.dump_model(txt_dump_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$txt_dump_model\"\n",
    "\n",
    "#cat $1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = 20,20\n",
    "plot_importance(booster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred = booster.predict(dtrain)\n",
    "train_predictions = np.array([value for value in train_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(pagantes_train_y, train_predictions.round())\n",
    "precision = precision_score(pagantes_train_y, train_predictions.round())\n",
    "recall = recall_score(pagantes_train_y, train_predictions.round())\n",
    "\n",
    "print_log(\"(Base Train)Clientes Total:{}\".format(len(train_predictions)))\n",
    "print_log(\"(Base Train)PAGANTES Previstos:{}\".format(len([x for x in train_predictions if x > 0.5])))\n",
    "print_log(\"(Base Train)PAGANTES na Base Train:{}\".format(len([x for x in pagantes_train_y if x > 0.5])))\n",
    "print_log(\"(Base Train)Accuracy Total:{}\".format(accuracy))\n",
    "print_log(\"(Base Train)Precision:{}\".format(precision))\n",
    "print_log(\"(Base Train)Recall:{}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pagantes_train_y.shape, train_predictions.shape)\n",
    "print(pagantes_train_y.dtype, train_predictions.dtype)\n",
    "pagantes_train_y = pagantes_train_y.astype('float32')\n",
    "train_predictions = train_predictions.astype('float32').round()\n",
    "\n",
    "#confusion_matrix = ConfusionMatrix(np.squeeze(pagantes_train_y), np.squeeze(train_predictions))\n",
    "#print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "#print(\"TP:{}\".format(confusion_matrix.TP.sum()))\n",
    "tn, fp, fn, tp = confusion_matrix(np.squeeze(pagantes_train_y), np.squeeze(train_predictions)).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(\"True Positive:{}\".format(tp))\n",
    "print(\"True Negative:{}\".format(tn))\n",
    "print(\"False Positive:{}\".format(fp))\n",
    "print(\"False Negative:{}\".format(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = booster.predict(dtest)\n",
    "test_predictions = np.array([value for value in test_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(pagantes_test_y, test_predictions.round())\n",
    "precision = precision_score(pagantes_test_y, test_predictions.round())\n",
    "recall = recall_score(pagantes_test_y, test_predictions.round())\n",
    "\n",
    "print_log(\"(Base Test)Clientes Total:{}\".format(len(test_predictions)))\n",
    "print_log(\"(Base Test)PAGANTES Previstos:{}\".format(len([x for x in test_predictions if x > 0.5])))\n",
    "print_log(\"(Base Test)PAGANTES na Base Teste:{}\".format(len([x for x in pagantes_test_y if x > 0.5])))\n",
    "print_log(\"(Base Test)Accuracy Total:{}\".format(accuracy))\n",
    "print_log(\"(Base Test)Precision:{}\".format(precision))\n",
    "print_log(\"(Base Test)Recall:{}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = \"../data/output/{}.model\".format(datetime.now().strftime(\"%Y%m%d.%H%M%S\"))\n",
    "with open(save_file, 'wb') as fp:\n",
    "    pickle.dump(booster, fp)    \n",
    "print_log(\"Model saved as {}\".format(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criacao de Modelo de Classificacao de Pagamentos para a Herval, usando XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import dateutil.parser as parser\n",
    "import os.path\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros de criacao da Arvore com XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HERVAL_DATA_FOLDER' in os.environ:\n",
    "    data_folder = os.environ['HERVAL_DATA_FOLDER']\n",
    "else:\n",
    "    data_folder = \"../../\"\n",
    "\n",
    "cluster = 2 # Cluster a ser utilizado\n",
    "proporcao_train = 0.7 # Proporcao de dados do dataframe para ser usado no treinamento\n",
    "proporcao_test = 0.3 # Proporcao de dados do dataframe para ser usado em validacao\n",
    "min_trees = 5 # Numero minimo de arvores de decisao a ser criados no ensemble\n",
    "max_trees = 20 # Numero maximo de arvores de decisao a ser criados no ensemble\n",
    "learning_rates_to_run = [0.3] # Learning Rate, ou seja, o grau de descida do nosso gradiente de erro\n",
    "depth_to_run = [3,5,10,20] # Profundidades maximas permitidas em cada arvore.\n",
    "\n",
    "log_location = data_folder + \"logs/\"\n",
    "arquivo_pagantes_norm = data_folder + \"data/batch03/intermediate/Herval.normalized.pickle\"\n",
    "arquivo_pagantes_norm_train_x = data_folder + \"data/batch03/intermediate/Herval.normalized.train.x.pickle\"\n",
    "\n",
    "txt_dump_model = data_folder + \"data/batch03/intermediate/model_generated.txt\"\n",
    "txt_feat_map_model = data_folder + \"data/batch03/intermediate/feat_map_generated.txt\"\n",
    "\n",
    "output_folder= data_folder + \"data/batch03/model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracao do Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'xgboost.log.' + datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))\n",
    "def print_log(msg):\n",
    "    logging.debug(msg)\n",
    "    print(msg)\n",
    "    \n",
    "def log(msg):\n",
    "    logging.debug(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento do Dataframe a ser utilizado no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Carregando Pickling normalizado:{}\".format(arquivo_pagantes_norm))    \n",
    "pagantes = pd.read_pickle(arquivo_pagantes_norm)\n",
    "pagantes = pagantes.query(\"CLUSTER == {}\".format(cluster))\n",
    "total_pagantes = len(pagantes.index)\n",
    "print_log(\"Total pagantes:{}\".format(total_pagantes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacao dos Conjuntos de Dados de Teste e de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Criando dataframes de train e teste...\")\n",
    "pagantes = pagantes.sample(int(len(pagantes.index)))\n",
    "pagantes_train = pagantes.tail(int(len(pagantes.index) * proporcao_train))\n",
    "pagantes_test = pagantes.head(int(len(pagantes.index) * proporcao_test))\n",
    "del pagantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criacao de uma referencia dos nomes das colunas a ser usado no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_reference(header_chamadas_x,arquivo_df_pickled_norm_train_x):\n",
    "    print_log(\"Criando Arquivo de referencia de colunas...\")\n",
    "    with open(arquivo_df_pickled_norm_train_x+\".txt\",\"w\") as f:\n",
    "        counter = 0\n",
    "        lista_header = list(header_chamadas_x.columns.values)\n",
    "        for header in lista_header:\n",
    "            f.write(\"{}-{}\\n\".format(counter,header))\n",
    "            counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_column_reference(pagantes_train.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA'].head(1), arquivo_pagantes_norm_train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversao dos dataframes Pandas para arrays do tipo Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes_train_x = pagantes_train.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA'] # Pegamos apenas as colunas que realmente sao as caracteristicas\n",
    "pagantes_train_y = pagantes_train.loc[:, 'PAGOU':'PAGOU'] # Colunas com as variaveis alvo\n",
    "\n",
    "pagantes_test_x = pagantes_test.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA'] # Pegamos apenas as colunas que realmente sao as caracteristicas\n",
    "pagantes_test_y = pagantes_test.loc[:, 'PAGOU':'PAGOU'] # Colunas com as variaveis alvo\n",
    "\n",
    "colunas_x = pagantes_train_x.columns.values # Nomes das Colunas com as caracteristicas\n",
    "colunas_y = pagantes_train_y.columns.values # Nome da Coluna Alvo\n",
    "\n",
    "pagantes_train_x = pagantes_train_x.as_matrix() # Conversao do Dataframe Pandas para Array Numpy\n",
    "pagantes_train_y = pagantes_train_y.as_matrix() # Conversao do Dataframe Pandas para Array Numpy\n",
    "\n",
    "pagantes_test_x = pagantes_test_x.as_matrix() # Conversao do Dataframe Pandas para Array Numpy\n",
    "pagantes_test_y = pagantes_test_y.as_matrix() # Conversao do Dataframe Pandas para Array Numpy\n",
    "\n",
    "colunas_x = [x for x in colunas_x] # Conversao para lista das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch = datetime.now().strftime(\"%Y%m%d.%H%M%S\")\n",
    "tested_hyper_parameters = []\n",
    "\n",
    "\"\"\"\n",
    "Funcao para criacao de booster, ou seja, um conjunto de arvores de decisao que forma um ensemble de acordo com uma\n",
    "serie de hyperparametros\n",
    "\"\"\"\n",
    "def create_booster(eta,depth,num_trees):\n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eta'] = eta\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['eval_metric'] = 'auc'\n",
    "    param['tree_method'] = 'auto'\n",
    "    param['silent'] = 0\n",
    "    param['max_depth'] = depth\n",
    "    param['subsample'] = 0.5\n",
    "    num_round = num_trees\n",
    "    dtrain = xgb.DMatrix(pagantes_train_x, pagantes_train_y, feature_names = colunas_x)\n",
    "    dtest = xgb.DMatrix(pagantes_test_x, pagantes_test_y, feature_names = colunas_x)\n",
    "    train_labels = dtrain.get_label()\n",
    "    ratio = float(np.sum(train_labels == 0)) / np.sum(train_labels == 1) \n",
    "    param['scale_pos_weight'] = ratio\n",
    "    gpu_res = {}\n",
    "    booster = xgb.train(param, dtrain, num_round, evals_result=gpu_res, evals = [])    \n",
    "    return booster, dtrain, dtest\n",
    "\n",
    "# Loopa para criar um modelo para cada combinacao dos parametros de hiperparametros\n",
    "for eta in learning_rates_to_run:\n",
    "    for depth in depth_to_run:\n",
    "        for num_trees in range(min_trees, max_trees):\n",
    "            gc.collect()\n",
    "            booster, dtrain, dtest = create_booster(eta,depth,num_trees)\n",
    "            booster.dump_model(os.path.join(output_folder,\"{}.{}.{}.txt\".format(model_batch,depth,num_trees)))\n",
    "            save_file = os.path.join(output_folder,\"{}.{}.{}.xgboost.model\".format(model_batch,depth,num_trees))\n",
    "            relevant_features = sorted( ((v,k) for k,v in booster.get_score().items()), reverse=True)\n",
    "\n",
    "            # Calcula a matriz de confusao de acordo com cada threshold esperado.\n",
    "            for current_threshold in range(0, 30):\n",
    "                # Calcula o threshold a ser usado\n",
    "                threshold = (0.5 - (current_threshold / 100))\n",
    "                # Calcula a performance a partir dos dados de treinamento\n",
    "                train_y_pred = booster.predict(dtrain)\n",
    "                train_predictions = np.array([value for value in train_y_pred])\n",
    "                train_predictions = np.array([1 if x > threshold else 0 for x in train_predictions])\n",
    "                pagantes_train_y = pagantes_train_y.astype('float32')\n",
    "                train_predictions = train_predictions.astype('float32').round()\n",
    "                total_pagantes_train = len([x for x in pagantes_train_y if x == 1])\n",
    "                tn, fp, fn, tp = confusion_matrix(np.squeeze(pagantes_train_y), np.squeeze(train_predictions)).ravel()\n",
    "                pagantes_perdidos_train = (fn / total_pagantes_train ) * 100\n",
    "                msg = \"(TRAIN)Number of Trees:{}, eta:{}, depth:{} threshold:{:5.2f} \".format(num_trees, eta, depth, threshold) + \"True Positive:{} True Negative:{} False Positive:{} False Negative:{},  {}% de pagantes perdidos no test\".format(tp, tn, fp, fn, pagantes_perdidos_train )\n",
    "                \n",
    "                # Calcula a performance a partir dos dados de teste\n",
    "                test_y_pred = booster.predict(dtest)\n",
    "                test_predictions = np.array([value for value in test_y_pred])\n",
    "                test_predictions = np.array([1 if x > threshold else 0 for x in test_predictions])\n",
    "                pagantes_test_y = pagantes_test_y.astype('float32')\n",
    "                test_predictions = test_predictions.astype('float32').round()\n",
    "                total_pagantes_test = len([x for x in pagantes_test_y if x == 1])\n",
    "                tn, fp, fn, tp = confusion_matrix(np.squeeze(pagantes_test_y), np.squeeze(test_predictions)).ravel()\n",
    "                pagantes_perdidos_test = (fn / total_pagantes_train ) * 100\n",
    "                msg = \"(TEST)Number of Trees:{}, eta:{}, depth:{} threshold:{:5.2f} \".format(num_trees, eta, depth, threshold) + \"True Positive:{} True Negative:{} False Positive:{} False Negative:{},  {}% de pagantes perdidos no test\".format(tp, tn, fp, fn, pagantes_perdidos_test )\n",
    "                \n",
    "                porcentagem_pagamentos = tp / (tp + fn)\n",
    "                base_para_trabalhar = (tp+fp) / (tp + tn + fn + fp)\n",
    "                #Criamos um dicionario de dados que sera a base da nossa planilha de excel \n",
    "                current_info = { \n",
    "                    \"0cluster\" : cluster,\n",
    "                    \"1model\" : save_file,\n",
    "                    \"2num_trees\" : num_trees,\n",
    "                    \"3eta\" : eta,\n",
    "                    \"4depth\" : depth,\n",
    "                    \"5relevante_features\" : str(relevant_features),\n",
    "                    \"6threshold\" : threshold,\n",
    "                    \"7test0_total_pagantes\" : total_pagantes_test,\n",
    "                    \"7test1_true_negative\" : tn,\n",
    "                    \"7test2_true_positive\" : tp,\n",
    "                    \"7test3_false_positive\" : fp,\n",
    "                    \"7test4_false_negative\" : fn,\n",
    "                    \"7test5_pagantes_perdidos\" : total_pagantes_test,\n",
    "                    \"8%_pagamentos\" : porcentagem_pagamentos,\n",
    "                    \"8%_base_para_trabalhar\" : base_para_trabalhar,\n",
    "                    \"9delta\" : porcentagem_pagamentos - base_para_trabalhar\n",
    "                }\n",
    "                tested_hyper_parameters.append(current_info)\n",
    "            # Salva o modelo\n",
    "            booster.save_model(save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criacao de uma planilha de excel apartir da lista de dicionarios populada com as informacoes dos modelos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tested_hyper_parameters).to_excel(os.path.join(output_folder,\"{}.model.xls\".format(model_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

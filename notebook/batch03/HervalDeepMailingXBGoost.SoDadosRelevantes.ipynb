{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import dateutil.parser as parser\n",
    "import os.path\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 2\n",
    "proporcao_train = 0.7\n",
    "proporcao_test = 0.3\n",
    "min_trees = 5\n",
    "max_trees = 20\n",
    "learning_rates_to_run = [0.3]\n",
    "depth_to_run = [3,5,10,20]\n",
    "\n",
    "log_location = \"../../logs/\"\n",
    "arquivo_pagantes_norm = \"../../data/batch03/intermediate/Herval.normalized.pickle\"\n",
    "arquivo_pagantes_norm_train_x = \"../../data/batch03/intermediate/Herval.normalized.train.x.pickle\"\n",
    "\n",
    "txt_dump_model = \"../../data/batch03/intermediate/model_generated.txt\"\n",
    "txt_feat_map_model = \"../../data/batch03/intermediate/feat_map_generated.txt\"\n",
    "\n",
    "output_folder= \"../../data/batch03/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'xgboost.log.' + datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(msg):\n",
    "    logging.debug(msg)\n",
    "    print(msg)\n",
    "    \n",
    "def log(msg):\n",
    "    logging.debug(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Carregando Pickling normalizado:{}\".format(arquivo_pagantes_norm))    \n",
    "pagantes = pd.read_pickle(arquivo_pagantes_norm)\n",
    "pagantes = pagantes.query(\"CLUSTER == {}\".format(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pagantes = len(pagantes.index)\n",
    "print_log(\"Total pagantes:{}\".format(total_pagantes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_reference(header_chamadas_x,arquivo_df_pickled_norm_train_x):\n",
    "    print_log(\"Criando Arquivo de referencia de colunas...\")\n",
    "    with open(arquivo_df_pickled_norm_train_x+\".txt\",\"w\") as f:\n",
    "        counter = 0\n",
    "        lista_header = list(header_chamadas_x.columns.values)\n",
    "        for header in lista_header:\n",
    "            f.write(\"{}-{}\\n\".format(counter,header))\n",
    "            counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(\"Criando dataframes de train e teste...\")\n",
    "pagantes = pagantes.sample(int(len(pagantes.index)))\n",
    "pagantes_train = pagantes.tail(int(len(pagantes.index) * proporcao_train))\n",
    "pagantes_test = pagantes.head(int(len(pagantes.index) * proporcao_test))\n",
    "del pagantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_column_reference(pagantes_train.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA'].head(1), arquivo_pagantes_norm_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagantes_train_x = pagantes_train.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA']\n",
    "pagantes_train_y = pagantes_train.loc[:, 'PAGOU':'PAGOU']\n",
    "\n",
    "pagantes_test_x = pagantes_test.loc[:, 'NORM_CLASSE_SOCIAL_A1':'NORM_RENDA_PRESUMIDA']\n",
    "pagantes_test_y = pagantes_test.loc[:, 'PAGOU':'PAGOU']\n",
    "\n",
    "colunas_x = pagantes_train_x.columns.values\n",
    "colunas_y = pagantes_train_y.columns.values\n",
    "\n",
    "pagantes_train_x = pagantes_train_x.as_matrix()\n",
    "pagantes_train_y = pagantes_train_y.as_matrix()\n",
    "\n",
    "pagantes_test_x = pagantes_test_x.as_matrix()\n",
    "pagantes_test_y = pagantes_test_y.as_matrix()\n",
    "\n",
    "colunas_x = [x for x in colunas_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msg1 = \"Train - Pagantes Detectados {} num universo de {}\".format(len([y for y in pagantes_train_y if y >0]),len(pagantes_train_y))\n",
    "#msg2 = \"Test - Pagantes Detectados {} num universo de {}\".format(len([y for y in pagantes_test_y if y >0]),len(pagantes_test_y))\n",
    "#print_log(msg1)\n",
    "#print_log(msg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pagantes = (len([y for y in pagantes_train_y if y >0]),len(pagantes_train_y))\n",
    "#print(pagantes)\n",
    "#ratio = pagantes[1] / pagantes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch = datetime.now().strftime(\"%Y%m%d.%H%M%S\")\n",
    "tested_hyper_parameters = []\n",
    "\n",
    "def create_booster(eta,depth,num_trees):\n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eta'] = eta\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['eval_metric'] = 'auc'\n",
    "    param['tree_method'] = 'auto'\n",
    "    param['silent'] = 0\n",
    "    param['max_depth'] = depth\n",
    "    param['subsample'] = 0.5\n",
    "    num_round = num_trees\n",
    "    dtrain = xgb.DMatrix(pagantes_train_x, pagantes_train_y, feature_names = colunas_x)\n",
    "    dtest = xgb.DMatrix(pagantes_test_x, pagantes_test_y, feature_names = colunas_x)\n",
    "    train_labels = dtrain.get_label()\n",
    "    ratio = float(np.sum(train_labels == 0)) / np.sum(train_labels == 1) \n",
    "    param['scale_pos_weight'] = ratio\n",
    "    gpu_res = {}\n",
    "    booster = xgb.train(param, dtrain, num_round, evals_result=gpu_res, evals = [])    \n",
    "    return booster, dtrain, dtest\n",
    "\n",
    "for eta in learning_rates_to_run:\n",
    "    for depth in depth_to_run:\n",
    "        for num_trees in range(min_trees, max_trees):\n",
    "            gc.collect()\n",
    "            booster, dtrain, dtest = create_booster(eta,depth,num_trees)\n",
    "            booster.dump_model(os.path.join(output_folder,\"{}.{}.{}.txt\".format(model_batch,depth,num_trees)))\n",
    "            save_file = os.path.join(output_folder,\"{}.{}.{}.xgboost.model\".format(model_batch,depth,num_trees))\n",
    "            relevant_features = sorted( ((v,k) for k,v in booster.get_score().items()), reverse=True)\n",
    "            #print_log(relevant_features)\n",
    "\n",
    "            for current_threshold in range(0, 30):\n",
    "                threshold = (0.5 - (current_threshold / 100))\n",
    "                train_y_pred = booster.predict(dtrain)\n",
    "                train_predictions = np.array([value for value in train_y_pred])\n",
    "                train_predictions = np.array([1 if x > threshold else 0 for x in train_predictions])\n",
    "                pagantes_train_y = pagantes_train_y.astype('float32')\n",
    "                train_predictions = train_predictions.astype('float32').round()\n",
    "                total_pagantes_train = len([x for x in pagantes_train_y if x == 1])\n",
    "                tn, fp, fn, tp = confusion_matrix(np.squeeze(pagantes_train_y), np.squeeze(train_predictions)).ravel()\n",
    "                pagantes_perdidos_train = (fn / total_pagantes_train ) * 100\n",
    "                msg = \"(TRAIN)Number of Trees:{}, eta:{}, depth:{} threshold:{:5.2f} \".format(num_trees, eta, depth, threshold) + \"True Positive:{} True Negative:{} False Positive:{} False Negative:{},  {}% de pagantes perdidos no test\".format(tp, tn, fp, fn, pagantes_perdidos_train )\n",
    "                #print_log(msg)\n",
    "\n",
    "                test_y_pred = booster.predict(dtest)\n",
    "                test_predictions = np.array([value for value in test_y_pred])\n",
    "                test_predictions = np.array([1 if x > threshold else 0 for x in test_predictions])\n",
    "                pagantes_test_y = pagantes_test_y.astype('float32')\n",
    "                test_predictions = test_predictions.astype('float32').round()\n",
    "                total_pagantes_test = len([x for x in pagantes_test_y if x == 1])\n",
    "                tn, fp, fn, tp = confusion_matrix(np.squeeze(pagantes_test_y), np.squeeze(test_predictions)).ravel()\n",
    "                pagantes_perdidos_test = (fn / total_pagantes_train ) * 100\n",
    "                msg = \"(TEST)Number of Trees:{}, eta:{}, depth:{} threshold:{:5.2f} \".format(num_trees, eta, depth, threshold) + \"True Positive:{} True Negative:{} False Positive:{} False Negative:{},  {}% de pagantes perdidos no test\".format(tp, tn, fp, fn, pagantes_perdidos_test )\n",
    "                \n",
    "                porcentagem_pagamentos = tp / (tp + fn)\n",
    "                base_para_trabalhar = (tp+fp) / (tp + tn + fn + fp)\n",
    "                current_info = { \n",
    "                    \"0cluster\" : cluster,\n",
    "                    \"1model\" : save_file,\n",
    "                    \"2num_trees\" : num_trees,\n",
    "                    \"3eta\" : eta,\n",
    "                    \"4depth\" : depth,\n",
    "                    \"5relevante_features\" : str(relevant_features),\n",
    "                    \"6threshold\" : threshold,\n",
    "                    \"7test0_total_pagantes\" : total_pagantes_test,\n",
    "                    \"7test1_true_negative\" : tn,\n",
    "                    \"7test2_true_positive\" : tp,\n",
    "                    \"7test3_false_positive\" : fp,\n",
    "                    \"7test4_false_negative\" : fn,\n",
    "                    \"7test5_pagantes_perdidos\" : total_pagantes_test,\n",
    "                    \"8%_pagamentos\" : porcentagem_pagamentos,\n",
    "                    \"8%_base_para_trabalhar\" : base_para_trabalhar,\n",
    "                    \"9delta\" : porcentagem_pagamentos - base_para_trabalhar\n",
    "                }\n",
    "                tested_hyper_parameters.append(current_info)\n",
    "            booster.save_model(save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tested_hyper_parameters).to_excel(os.path.join(output_folder,\"{}.model.xls\".format(model_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
